# Traffic Sign Guided Autonomous Robot Navigation with SLAM

**Team 10 - Final Project**

A ROS-based autonomous mobile robot navigation system that uses computer vision for traffic sign detection and SLAM algorithms for mapping and localization in a simulated office environment.

> **Robot URDF Model**: [Pioneer 3-AT Description](https://github.com/MobileRobots/amr-ros-config/tree/master/description)

## ğŸ‘¥ Team Members
- Yunus Emre Kuru
- AkÄ±n MÄ±rÄ±k
- Muhammet MÃ¼ftÃ¼oÄŸlu

## ğŸ“‹ Table of Contents
- [Overview](#-overview)
- [Features](#-features)
- [System Architecture](#ï¸-system-architecture)
- [Hardware Specifications](#-hardware-specifications)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [SLAM Comparison Results](#-slam-comparison-results)
- [Traffic Sign Detection](#-traffic-sign-detection)
- [Demo Videos](#-demo-videos)
- [Troubleshooting](#-troubleshooting)
- [References](#-references)

## ğŸ¯ Overview

This project implements an autonomous navigation system for a Pioneer 3-AT mobile robot in a Gazebo-simulated AWS Office environment. The robot uses:

- **Computer Vision** - Detects traffic signs (turn left, turn right, go straight, park) using OpenCV template matching
- **SLAM Algorithms** - Creates 2D occupancy grid maps using GMapping and Hector SLAM
- **Reactive Navigation** - Wall-following algorithm with LiDAR-based obstacle avoidance and traffic sign control
- **Sensor Fusion** - Integrates dual LiDAR sensors, RGB camera, and wheel odometry

### Project Objectives

1. âœ… Add traffic signs to Gazebo world (left, right, straight, park)
2. âœ… Implement traffic sign detector using RGB camera (OpenCV)
3. âœ… Guide robot through environment with global/local planner and tracker
4. âœ… Run SLAM algorithms to create 2D maps:
   - Hector SLAM
   - GMapping
5. âœ… Compare algorithms qualitatively and quantitatively:
   - Output map quality
   - Localization performance vs. ground truth

## âœ¨ Features

- **Autonomous Exploration**: Wall-following algorithm with dynamic obstacle avoidance
- **Traffic Sign Recognition**: Real-time detection with 65% confidence threshold
- **Dual SLAM Implementation**: Compare GMapping vs Hector SLAM performance
- **Reactive Navigation**: Distance-based speed adjustment (0.15-0.55 m/s)
- **Parking Mode**: Automated parking at 1.4m from detected obstacles
- **Performance Metrics**: Real-time RMSE calculation against ground truth
- **Multi-sensor Fusion**: Front/rear LiDAR + RGB camera + odometry

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Gazebo Simulation                        â”‚
â”‚              (AWS Office World + Pioneer 3-AT)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ LiDAR  â”‚        â”‚  RGB Camera  â”‚  â”‚ Odom   â”‚
â”‚ F + R  â”‚        â”‚ (12.3 MP)    â”‚  â”‚        â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                    â”‚              â”‚
    â”‚ /laser_scan        â”‚ /image_raw   â”‚
    â”‚                    â”‚              â”‚
    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚            â”‚ Traffic Detector â”‚   â”‚
    â”‚            â”‚  (OpenCV)        â”‚   â”‚
    â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                    â”‚              â”‚
    â”‚                    â”‚ /traffic_signâ”‚
    â”‚                    â”‚              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚
    â”‚           â”‚    SLAM    â”‚         â”‚
    â”‚           â”‚  Gmapping/ â”‚         â”‚
    â”‚           â”‚   Hector   â”‚         â”‚
    â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚                  â”‚ /map          â”‚
    â”‚                  â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                â”‚   Navigator    â”‚â—„â”€â”€â”€â”¤
                â”‚(Wall-Following)â”‚â—„â”€â”€â”€â”˜ /odom
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ /sim_p3at/cmd_vel
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Robot Motion    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Architecture Notes:**
- **LiDAR** provides data to three components in parallel:
  - SLAM Engines (for mapping)
  - Navigator (for wall-following & obstacle avoidance)
  - Traffic Detector (for context)
- **Navigator** receives inputs from:
  - LiDAR (`/front_laser/scan`) - obstacle detection
  - Traffic signs (`/traffic_sign`) - navigation commands
  - Odometry (`/odom`) - position tracking
- Navigator uses **wall-following algorithm** with direct sensor feedback (no path planner)
- SLAM provides mapping for visualization; navigator operates reactively

## ğŸ¤– Hardware Specifications

### Robot Platform: Pioneer 3-AT

| Component | Specification |
|-----------|---------------|
| **Type** | All-Terrain Differential Drive Mobile Robot |
| **Dimensions** | 52cm (W) Ã— 56cm (L) footprint |
| **Mass** | 21.5 kg |
| **Drive** | 4-wheel differential drive with skid steering |
| **Max Linear Speed** | 0.35 m/s (hardware limit)<br>0.15-0.55 m/s (software operational range) |
| **Max Angular Speed** | 0.8 rad/s |

### Sensors

| Sensor | Model/Specs | Purpose |
|--------|-------------|---------|
| **Front LiDAR** | 2D Laser Scanner<br>â€¢ 270Â° FOV<br>â€¢ 20m range<br>â€¢ 540 samples<br>â€¢ 0.5Â° resolution | Primary obstacle detection & SLAM |
| **Rear LiDAR** | Same as front | Rear obstacle detection & complete coverage |
| **RGB Camera** | Flir BlackflyS BFS-U3-122S6C-C<br>â€¢ 12.3 MP<br>â€¢ 23 FPS<br>â€¢ 60Â° FOV | Traffic sign detection |
| **Wheel Encoders** | Built-in | Odometry & dead reckoning |

### Simulation Environment

- **World**: AWS Office (from [mlherd/Dataset-of-Gazebo-Worlds-Models-and-Maps](https://github.com/mlherd/Dataset-of-Gazebo-Worlds-Models-and-Maps))
- **Physics Engine**: ODE (Open Dynamics Engine)
- **Update Rate**: 500 Hz
- **Environment Size**: ~30m Ã— 30m indoor office space
- **Features**: Multiple rooms, hallways, corridors, office furniture, traffic signs

## ğŸ”§ Installation

### Prerequisites

```bash
# Ubuntu 20.04 LTS
# ROS Noetic (or ROS Melodic for Ubuntu 18.04)
# Gazebo 11
# Python 3.8+
```

### Dependencies

```bash
# Install ROS Noetic (if not already installed)
sudo apt update
sudo apt install ros-noetic-desktop-full

# Install required ROS packages
sudo apt install ros-noetic-gazebo-ros-pkgs
sudo apt install ros-noetic-navigation
sudo apt install ros-noetic-slam-gmapping
sudo apt install ros-noetic-hector-slam
sudo apt install ros-noetic-move-base  # Required by launch file but not used by navigator
sudo apt install ros-noetic-amcl
sudo apt install ros-noetic-map-server

# Install Python dependencies
sudo apt install python3-pip
pip3 install opencv-python
pip3 install numpy
pip3 install rospy
```

### Setup

```bash
# Clone the repository
cd ~/Desktop
git clone https://github.com/muhammet-mft/traffic-sign-robot-navigation.git
cd traffic-sign-robot-navigation

# Create catkin workspace (if needed)
mkdir -p ~/catkin_ws/src
cd ~/catkin_ws/src
ln -s ~/Desktop/traffic-sign-robot-navigation/Files/amr-ros-config .

# Build the workspace
cd ~/catkin_ws
catkin_make

# Source the workspace
echo "source ~/catkin_ws/devel/setup.bash" >> ~/.bashrc
source ~/.bashrc

# Copy Gazebo models to Gazebo model path
mkdir -p ~/.gazebo/models
cp -r Files/amr-ros-config/gazebo/models/* ~/.gazebo/models/
```

## ğŸš€ Usage

### Quick Start - Complete System (with Hector SLAM)

#### Terminal 1: Launch Gazebo World and Robot
```bash
roslaunch amr-ros-config project_world.launch
```

#### Terminal 2: Reset Robot Position
```bash
rosrun amr-ros-config reset_robot.py
```

#### Terminal 3: Start Traffic Sign Detector
```bash
rosrun amr-ros-config traffic_detector_2.py
```

#### Terminal 4: Launch Navigation Stack
```bash
roslaunch traffic_exploration.launch
```

#### Terminal 5: Launch Hector SLAM
```bash
roslaunch amr-ros-config slam_hector.launch
```

*Note: In RViz, Add -> Map and use topic `/map` to visualize mapping*

#### Terminal 6: Start Autonomous Navigator
```bash
rosrun amr-ros-config navigator_3.py
```

#### Terminal 7 (Optional): Monitor Position Accuracy
```bash
rosrun amr-ros-config coordinate_viewer.py
```

---

### Option A: Using GMapping

**See detailed instructions in:** `Files/gMapping/How to use codes.txt`

#### Terminal 1: Launch Gazebo World and Robot
```bash
roslaunch amr-ros-config project_world.launch
```

#### Terminal 2: Reset Robot Position
```bash
rosrun amr-ros-config reset_robot.py
```

#### Terminal 3: Start Traffic Sign Detector
```bash
rosrun amr-ros-config traffic_detector_2.py
```

#### Terminal 4: Launch GMapping SLAM
```bash
roslaunch amr-ros-config slam_gmapping.launch
```

*Note: In RViz, Add -> Map and use topic `/map` to visualize mapping*

#### Terminal 5: Start Autonomous Navigator
```bash
rosrun amr-ros-config navigator_3.py
```

#### Terminal 6 (Optional): Save Map
```bash
rosrun map_server map_saver -f gmapping_map
```

---

### Option B: Using Hector SLAM (Simplified - Without Navigation Stack)

**See detailed instructions in:** `Files/Hector/How to use codes.txt`

#### Terminal 1: Launch Gazebo World and Robot
```bash
roslaunch amr-ros-config project_world.launch
```

#### Terminal 2: Reset Robot Position
```bash
rosrun amr-ros-config reset_robot.py
```

#### Terminal 3: Start Traffic Sign Detector
```bash
rosrun amr-ros-config traffic_detector_2.py
```

#### Terminal 4: Launch Hector SLAM
```bash
roslaunch amr-ros-config slam_hector.launch
```

*Note: In RViz, Add -> Map and use topic `/map` to visualize mapping*

#### Terminal 5: Start Autonomous Navigator
```bash
rosrun amr-ros-config navigator_3.py
```

#### Terminal 6 (Optional): Save Map
```bash
rosrun map_server map_saver -f hector_map
```

---

### Visualization

RViz is automatically launched with the world. You can visualize:
- Robot model
- LaserScan data
- Generated maps
- Planned paths
- Costmaps
- Camera feed
- TF transforms

## ğŸ“ Project Structure

```
traffic-sign-robot-navigation/
â”œâ”€â”€ README.md
â”œâ”€â”€ gmapping_haritam.pgm           # Generated GMapping map (16 MB)
â”œâ”€â”€ hector_haritam.pgm             # Generated Hector map (4 MB)
â”œâ”€â”€ Gmapping_Error.jpeg            # GMapping error logs
â”œâ”€â”€ Hector_Error.jpeg              # Hector error logs
â”‚
â””â”€â”€ Files/
    â”œâ”€â”€ office.jpg                 # AWS Office environment reference image
    â”‚
    â”œâ”€â”€ amr-ros-config/            # Main ROS package
    â”‚   â”œâ”€â”€ launch/                # Launch files
    â”‚   â”‚   â”œâ”€â”€ project_world.launch         # Main world + robot
    â”‚   â”‚   â”œâ”€â”€ slam_gmapping.launch         # GMapping SLAM
    â”‚   â”‚   â”œâ”€â”€ slam_hector.launch           # Hector SLAM
    â”‚   â”‚   â””â”€â”€ traffic_exploration.launch   # Navigation stack
    â”‚   â”‚
    â”‚   â”œâ”€â”€ scripts/               # Python scripts
    â”‚   â”‚   â”œâ”€â”€ navigator_3.py               # Main navigation controller
    â”‚   â”‚   â”œâ”€â”€ traffic_detector_2.py        # Traffic sign detector
    â”‚   â”‚   â”œâ”€â”€ coordinate_viewer.py         # RMSE calculator
    â”‚   â”‚   â”œâ”€â”€ reset_robot.py               # Robot reset utility
    â”‚   â”‚   â””â”€â”€ move_robot.py                # Movement test script
    â”‚   â”‚
    â”‚   â”œâ”€â”€ nav_config/            # Navigation parameters
    â”‚   â”‚   â”œâ”€â”€ costmap_common_params.yaml
    â”‚   â”‚   â”œâ”€â”€ local_costmap_params.yaml
    â”‚   â”‚   â”œâ”€â”€ global_costmap_params.yaml
    â”‚   â”‚   â””â”€â”€ base_local_planner_params.yaml
    â”‚   â”‚
    â”‚   â”œâ”€â”€ description/           # Robot URDF models
    â”‚   â”‚   â””â”€â”€ urdf/
    â”‚   â”‚       â””â”€â”€ pioneer3at_lidar_rgb.urdf
    â”‚   â”‚
    â”‚   â””â”€â”€ gazebo/                # Gazebo simulation environment
    â”‚       â”œâ”€â”€ final_Added.world           # Main simulation world (1.2 MB)
    â”‚       â”œâ”€â”€ models/                     # 43 models total
    â”‚       â”‚   â”œâ”€â”€ sign_left/              # Traffic signs (4)
    â”‚       â”‚   â”œâ”€â”€ sign_right/
    â”‚       â”‚   â”œâ”€â”€ sign_straight/
    â”‚       â”‚   â”œâ”€â”€ sign_park/
    â”‚       â”‚   â””â”€â”€ ... (39 office objects: furniture, actors, etc.)
    â”‚       â”œâ”€â”€ map/                        # Ground truth maps
    â”‚       â””â”€â”€ media/                      # Textures and materials
    â”‚
    â”œâ”€â”€ Signs_Models/             # Standalone traffic sign models
    â”‚   â”œâ”€â”€ sign_left/
    â”‚   â”œâ”€â”€ sign_right/
    â”‚   â”œâ”€â”€ sign_straight/
    â”‚   â””â”€â”€ sign_park/
    â”‚
    â”œâ”€â”€ gMapping/                 # GMapping-specific implementation
    â”‚   â”œâ”€â”€ How to use codes.txt
    â”‚   â”œâ”€â”€ navigator_3.py
    â”‚   â”œâ”€â”€ traffic_detector_2.py
    â”‚   â”œâ”€â”€ coordinate_viewer.py
    â”‚   â”œâ”€â”€ reset_robot.py
    â”‚   â””â”€â”€ slam_gmapping.launch
    â”‚
    â””â”€â”€ Hector/                   # Hector-specific implementation
        â”œâ”€â”€ How to use codes.txt
        â”œâ”€â”€ navigator_3.py
        â”œâ”€â”€ traffic_detector_2.py
        â”œâ”€â”€ coordinate_viewer.py
        â”œâ”€â”€ reset_robot.py
        â”œâ”€â”€ slam_hector.launch
        â””â”€â”€ traffic_exploration.launch
```

## ğŸ“Š SLAM Comparison Results

### Quantitative Analysis

| Metric | GMapping | Hector SLAM | Winner |
|--------|----------|-------------|--------|
| **Map Resolution** | 4000Ã—4000 px | 2048Ã—2048 px | **GMapping** âœ“ |
| **Map File Size** | 16 MB | 4 MB | Hector (more efficient) |
| **Average RMSE** | ~0.403-0.409 | ~0.041 | **Hector** âœ“ |
| **Computational Load** | Medium | Low | **Hector** âœ“ |
| **Map Update Rate** | 1 Hz | Real-time | **Hector** âœ“ |
| **Requires Odometry** | Yes | No | **Hector** âœ“ |

### Qualitative Observations

#### GMapping
**Strengths:**
- Higher resolution maps (4000Ã—4000)
- Better for larger environments
- More detailed obstacle representation
- Particle filter handles ambiguity well

**Weaknesses:**
- Higher computational cost (30 particles)
- Requires good odometry
- Slower map updates (1s intervals)
- Higher RMSE in our tests (~0.403-0.409)

#### Hector SLAM
**Strengths:**
- Excellent localization accuracy (RMSE ~0.041)
- No odometry required (scan-matching based)
- Real-time map updates
- Lower computational requirements
- Works well in structured environments

**Weaknesses:**
- Lower map resolution (2048Ã—2048)
- May struggle in feature-poor areas
- Less robust to fast rotations
- Smaller map size limit

### Generated Maps

Below are the actual maps generated by both SLAM algorithms during our autonomous exploration:

#### GMapping Output
![GMapping Map](gmapping_haritam.pgm)
*GMapping generated map (16 MB, 4000Ã—4000 px) - Higher resolution with more detail*

#### Hector SLAM Output
![Hector SLAM Map](hector_haritam.pgm)
*Hector SLAM generated map (4 MB, 2048Ã—2048 px) - Lighter weight with excellent accuracy*

### Conclusion
For this office environment navigation task, **Hector SLAM significantly outperformed GMapping** with ~10Ã— better RMSE (0.041 vs 0.403-0.409). Hector's scan-matching approach proved more suitable for structured indoor environments with good geometric features.

## ğŸš¦ Traffic Sign Detection

### Sign Types

| Sign | Visual | Description | Robot Response |
|------|--------|-------------|----------------|
| **LEFT** | ![Left](Files/Signs_Models/sign_left/materials/textures/left.jpeg) | Blue circle, white left arrow | Turns left at next intersection |
| **RIGHT** | ![Right](Files/Signs_Models/sign_right/materials/textures/right.jpeg) | Blue circle, white right arrow | Turns right at next intersection |
| **STRAIGHT** | ![Straight](Files/Signs_Models/sign_straight/materials/textures/straight.jpeg) | Blue circle, white up arrow | Continues straight ahead |
| **PARK** | ![Park](Files/Signs_Models/sign_park/materials/textures/park.jpeg) | Blue circle, "P" symbol | Approaches to 1.4m and stops |

### Detection Algorithm

**Method**: OpenCV Template Matching
- **Algorithm**: `cv2.matchTemplate()` with `TM_CCOEFF_NORMED`
- **Confidence Threshold**: 0.65 (65%)
- **Input**: `/rgb_camera/image_raw` (640Ã—480 @ 23 FPS)
- **Output**: `/traffic_sign` topic (String messages)
- **Processing**: Real-time with live video display

### Implementation Details

```python
# From traffic_detector_2.py
def image_callback(self, data):
    # Convert ROS Image to OpenCV format
    cv_image = self.bridge.imgmsg_to_cv2(data, "bgr8")
    gray_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)

    detected_sign = "NONE"
    max_val_all = 0

    # Template matching for each sign type
    for name, template in self.templates.items():
        res = cv2.matchTemplate(gray_image, template, cv2.TM_CCOEFF_NORMED)
        _, max_val, _, max_loc = cv2.minMaxLoc(res)

        if max_val > 0.65:  # Confidence threshold
            if max_val > max_val_all:
                max_val_all = max_val
                detected_sign = name

    if detected_sign != "NONE":
        self.sign_pub.publish(detected_sign)
```

### Navigation Response

**Note**: The code below is simplified pseudocode for illustration. The actual implementation in `navigator_3.py` includes additional logic for LiDAR-based distance checking, turn duration control (3 seconds), stored command management, and real-time obstacle avoidance.

```python
# From navigator_3.py (Simplified)
if detected_sign == "LEFT":
    turn_left()
elif detected_sign == "RIGHT":
    turn_right()
elif detected_sign == "STRAIGHT":
    continue_forward()
elif detected_sign == "PARK":
    park_mode(distance=1.4)  # Stop at 1.4m
```

## ğŸ¥ Demo Videos

**ğŸ“¹ [View All Demo Videos on Google Drive](https://drive.google.com/drive/folders/1C9OPpHTNZPf5NemMt2ehJV0YITXUk2D9)**

The demo folder includes:
- âœ… Full autonomous exploration with GMapping
- âœ… Full autonomous exploration with Hector SLAM
- âœ… Traffic sign detection demo
- âœ… Parking behavior demo
- âœ… SLAM comparison visualization

## ğŸ› Troubleshooting

### Common Issues

#### Issue: "Unable to load Gazebo models"
**Solution:**
```bash
# Add model path to environment
export GAZEBO_MODEL_PATH=~/.gazebo/models:$GAZEBO_MODEL_PATH
echo "export GAZEBO_MODEL_PATH=~/.gazebo/models:$GAZEBO_MODEL_PATH" >> ~/.bashrc
```

#### Issue: "No LaserScan data received"
**Solution:**
- Check that Gazebo is fully loaded (wait 10-15 seconds)
- Verify sensor topics: `rostopic list | grep laser`
- Check sensor connections: `rosnode info /gazebo`

#### Issue: "Traffic sign detector shows no window"
**Solution:**
```bash
# Install OpenCV with GUI support (remove headless version)
pip3 uninstall opencv-python-headless
pip3 install opencv-python
pip3 install opencv-contrib-python
```

#### Issue: "Robot doesn't move"
**Solution:**
1. Reset robot: `rosrun amr-ros-config reset_robot.py`
2. Check velocity topic: `rostopic echo /cmd_vel`
3. Verify navigator is running: `rosnode list | grep navigator`

#### Issue: "SLAM map is empty or distorted"
**Solution:**
- Ensure robot is moving (SLAM needs motion)
- Check LaserScan range (should see data in RViz)
- Verify TF transforms: `rosrun tf view_frames`
- Increase particle count (GMapping) or decrease resolution (Hector)

#### Issue: "High localization error (RMSE)"
**Solution:**
- Run calibration longer (coordinate_viewer.py needs 5+ seconds)
- Ensure odometry is publishing correctly
- Check for wheel slippage in simulation
- Verify sensor synchronization

## ğŸ“š References

### ROS Packages Used
- [navigation](http://wiki.ros.org/navigation) - ROS Navigation Stack
- [gmapping](http://wiki.ros.org/gmapping) - OpenSlam GMapping
- [hector_slam](http://wiki.ros.org/hector_slam) - Hector SLAM
- [gazebo_ros_pkgs](http://wiki.ros.org/gazebo_ros_pkgs) - Gazebo-ROS integration
- [pioneer_description](https://github.com/MobileRobots/amr-ros-config) - Pioneer robot models

### Resources
- [AWS Office World](https://github.com/mlherd/Dataset-of-Gazebo-Worlds-Models-and-Maps)
- [ROS Navigation Tuning Guide](http://wiki.ros.org/navigation/Tutorials)
- [OpenCV Template Matching](https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html)

### Papers & Documentation
- **GMapping**: Grisetti, G., Stachniss, C., & Burgard, W. (2007). "Improved Techniques for Grid Mapping with Rao-Blackwellized Particle Filters." *IEEE Transactions on Robotics*, 23(1), 34-46. [Paper](https://ieeexplore.ieee.org/document/4084563/)
- **Hector SLAM**: Kohlbrecher, S., von Stryk, O., Meyer, J., & Klingauf, U. (2011). "A Flexible and Scalable SLAM System with Full 3D Motion Estimation." *IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)*. [ROS Wiki](http://wiki.ros.org/hector_slam)
- **Pioneer 3-AT**: [ROS Pioneer 3-AT Documentation](https://robots.ros.org/pioneer-3-at/)

## ğŸ“„ License

This project was developed as an academic assignment. Feel free to use it for educational purposes.

## ğŸ™ Acknowledgments

- Course instructors and teaching assistants
- ROS community for excellent documentation
- AWS for the office world model
- OpenCV community for computer vision tools

---

**Project Status**: âœ… Completed (January 2026)

**Contact**: Team 10 Members (see above)

**Institution**: Istanbul Technical University

**Course**: KON 414E - Principles of Robot Autonomy
